{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation \n",
    "\n",
    "Here we generate simulated data to compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import Entrez, SeqIO\n",
    "Entrez.email = 'rohanarora@berkeley.edu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "percents = [1, 0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "\n",
    "for i in percents:\n",
    "    x = np.random.choice([0, 1], size=(8*1024*1024*100), replace=True, p=[i, (1-i)])\n",
    "    x = np.packbits(myvar)\n",
    "    open(str(int(i*100)), 'wb').write(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then DNA and Protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_seq = np.random.choice(['A','C','G','T'], size=100000000, replace=True)\n",
    "open('dna_seq.fa', 'w').write(''.join(dna_seq))\n",
    "\n",
    "protein_seq =np.random.choice(['A','R','N','D','B','C','E','Q','Z','G','H','I','L','K','M','F','P','S','T','W','Y','V'], size=100000000, replace=True)\n",
    "open('protein_seq.fa', 'w').write(''.join(protein_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Compression \n",
    "\n",
    "I have generated a table below to demonstrate the times it took to compress each one of my files with either the gzip, pbzip2, or bzip2 methods, as well as the resulting file sizes. The commands I used were:\n",
    "\n",
    "time gzip –c 'FILENAME' > 'FILENAME'.gz\n",
    "\n",
    "time bzip2 –k 'FILENAME'\n",
    "\n",
    "time pbzip2 –k 'FILENAME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Method</th>\n",
       "      <th>Input Size</th>\n",
       "      <th>Output Size</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>gzip</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>102 kB</td>\n",
       "      <td>0.624s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>bzip2</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>113 B</td>\n",
       "      <td>0.930s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>pbzip2</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>5.62 kB</td>\n",
       "      <td>0.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90</td>\n",
       "      <td>gzip</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>58.7 MB</td>\n",
       "      <td>27.024s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>bzip2</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>61.2 MB</td>\n",
       "      <td>10.227s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>90</td>\n",
       "      <td>pbzip2</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>61.2 MB</td>\n",
       "      <td>1.158s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>80</td>\n",
       "      <td>gzip</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>81.2 MB</td>\n",
       "      <td>18.237s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80</td>\n",
       "      <td>bzip2</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>86.6 MB</td>\n",
       "      <td>10.742s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>80</td>\n",
       "      <td>pbzip2</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>86.7 MB</td>\n",
       "      <td>1.371s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>70</td>\n",
       "      <td>gzip</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>93.6 MB</td>\n",
       "      <td>7.307s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>70</td>\n",
       "      <td>bzip2</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>99.8 MB</td>\n",
       "      <td>11.564s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>70</td>\n",
       "      <td>pbzip2</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>99.8 MB</td>\n",
       "      <td>1.585s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>60</td>\n",
       "      <td>gzip</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>102 MB</td>\n",
       "      <td>4.527s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>60</td>\n",
       "      <td>bzip2</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>12.809s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>60</td>\n",
       "      <td>pbzip2</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>1.899s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50</td>\n",
       "      <td>gzip</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>3.633s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>50</td>\n",
       "      <td>bzip2</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>13.615s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>50</td>\n",
       "      <td>pbzip2</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>105 MB</td>\n",
       "      <td>1.875s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dna_seq.fa</td>\n",
       "      <td>gzip</td>\n",
       "      <td>100 MB</td>\n",
       "      <td>29.2 MB</td>\n",
       "      <td>23.221s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dna_seq.fa</td>\n",
       "      <td>bzip2</td>\n",
       "      <td>100 MB</td>\n",
       "      <td>27.3 MB</td>\n",
       "      <td>9.339s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dna_seq.fa</td>\n",
       "      <td>pbzip2</td>\n",
       "      <td>100 MB</td>\n",
       "      <td>27.3 MB</td>\n",
       "      <td>1.097s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>protein_seq.fa</td>\n",
       "      <td>gzip</td>\n",
       "      <td>100 MB</td>\n",
       "      <td>61.8 MB</td>\n",
       "      <td>4.133s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>protein_seq.fa</td>\n",
       "      <td>bzip2</td>\n",
       "      <td>100 MB</td>\n",
       "      <td>57 MB</td>\n",
       "      <td>9.441s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>protein_seq.fa</td>\n",
       "      <td>pbzip2</td>\n",
       "      <td>100 MB</td>\n",
       "      <td>57 MB</td>\n",
       "      <td>1.114s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              File  Method Input Size Output Size     Time\n",
       "0              100    gzip     105 MB      102 kB   0.624s\n",
       "1              100   bzip2     105 MB       113 B   0.930s\n",
       "2              100  pbzip2     105 MB     5.62 kB    0.153\n",
       "3               90    gzip     105 MB     58.7 MB  27.024s\n",
       "4               90   bzip2     105 MB     61.2 MB  10.227s\n",
       "5               90  pbzip2     105 MB     61.2 MB   1.158s\n",
       "6               80    gzip     105 MB     81.2 MB  18.237s\n",
       "7               80   bzip2     105 MB     86.6 MB  10.742s\n",
       "8               80  pbzip2     105 MB     86.7 MB   1.371s\n",
       "9               70    gzip     105 MB     93.6 MB   7.307s\n",
       "10              70   bzip2     105 MB     99.8 MB  11.564s\n",
       "11              70  pbzip2     105 MB     99.8 MB   1.585s\n",
       "12              60    gzip     105 MB      102 MB   4.527s\n",
       "13              60   bzip2     105 MB      105 MB  12.809s\n",
       "14              60  pbzip2     105 MB      105 MB   1.899s\n",
       "15              50    gzip     105 MB      105 MB   3.633s\n",
       "16              50   bzip2     105 MB      105 MB  13.615s\n",
       "17              50  pbzip2     105 MB      105 MB   1.875s\n",
       "18      dna_seq.fa    gzip     100 MB     29.2 MB  23.221s\n",
       "19      dna_seq.fa   bzip2     100 MB     27.3 MB   9.339s\n",
       "20      dna_seq.fa  pbzip2     100 MB     27.3 MB   1.097s\n",
       "21  protein_seq.fa    gzip     100 MB     61.8 MB   4.133s\n",
       "22  protein_seq.fa   bzip2     100 MB       57 MB   9.441s\n",
       "23  protein_seq.fa  pbzip2     100 MB       57 MB   1.114s"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['File', 'Method', 'Input Size', 'Output Size', 'Time']\n",
    "data = [['100','gzip', '105 MB', '102 kB', '0.624s'],\n",
    "        ['100','bzip2', '105 MB', '113 B', '0.930s'],\n",
    "        ['100','pbzip2', '105 MB', '5.62 kB', '0.153'],\n",
    "        ['90','gzip', '105 MB', '58.7 MB', '27.024s'],\n",
    "        ['90','bzip2', '105 MB', '61.2 MB', '10.227s'],\n",
    "        ['90','pbzip2', '105 MB', '61.2 MB', '1.158s'],\n",
    "        ['80','gzip', '105 MB', '81.2 MB', '18.237s'],\n",
    "        ['80','bzip2', '105 MB', '86.6 MB', '10.742s'],\n",
    "        ['80','pbzip2', '105 MB', '86.7 MB', '1.371s'],\n",
    "        ['70','gzip', '105 MB', '93.6 MB', '7.307s'],\n",
    "        ['70','bzip2', '105 MB', '99.8 MB', '11.564s'],\n",
    "        ['70','pbzip2', '105 MB', '99.8 MB', '1.585s'],\n",
    "        ['60','gzip', '105 MB', '102 MB', '4.527s'],\n",
    "        ['60','bzip2', '105 MB', '105 MB', '12.809s'],\n",
    "        ['60','pbzip2', '105 MB', '105 MB', '1.899s'],\n",
    "        ['50','gzip', '105 MB', '105 MB', '3.633s'],\n",
    "        ['50','bzip2', '105 MB', '105 MB', '13.615s'],\n",
    "        ['50','pbzip2', '105 MB', '105 MB', '1.875s'],\n",
    "        ['dna_seq.fa','gzip', '100 MB', '29.2 MB', '23.221s'],\n",
    "        ['dna_seq.fa','bzip2', '100 MB', '27.3 MB', '9.339s'],\n",
    "        ['dna_seq.fa','pbzip2', '100 MB', '27.3 MB', '1.097s'],\n",
    "        ['protein_seq.fa','gzip', '100 MB', '61.8 MB', '4.133s'],\n",
    "        ['protein_seq.fa','bzip2', '100 MB', '57 MB', '9.441s'],\n",
    "        ['protein_seq.fa','pbzip2', '100 MB', '57 MB', '1.114s']]\n",
    "\n",
    "df = pd.DataFrame(data=data, columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Which algorithm achieves the best level of compression on each file type?** For the DNA and protein data the best level of compression is achieved by bzip2 and pbzip2. For binary data, it becomes more difficult to compress the as the % of 0s decreases. At around 50% none of the algorithms are that great. For the other cases, gzip seems to work best, except for th 100% 0s case, in which bzip2 is able to compress down to a mere number of bytes. \n",
    "\n",
    "**Which algorithm is the fastest?** pBzip2 is the fastest algorithm \n",
    "\n",
    "**What is the difference between bzip2 and pbzip2? Do you expect one to be faster and why?** Both are the same algorithm, however pbzip2 is able to make use of multiple CPU processing threads, and should therefore be faster. \n",
    "\n",
    "**How does the level of compression change as the percentage of zeros increases? Why does this happen?** As the percentage of zeros increases, the compressed file sizes are smaller (i.e. the compression is better). Files that have more 0s have a higher data entropy tand therefore there is more information for the algorithm to take care of. \n",
    "\n",
    "**What is the minimum number of bits required to store a single DNA base?** 2 bits (i.e. log2 4)\n",
    "\n",
    "**What is the minimum number of bits required to store an amino acid letter?** 5 bits (i.e. log2 20 rounded up)\n",
    "\n",
    "**In your tests, how many bits did gzip and bzip2 actually require to store your random DNA and protein sequences?**\n",
    "\n",
    "For DNA 2.86 bits on average per nucleic acid. \n",
    "For protein: 5.97 bits on average per residue\n",
    "\n",
    "**Are gzip and bzip2 performing well on DNA and proteins?** Yes, both types of data are also randomly distributed, but the compression times for gzip and bzip2 for either is less than the runtime for the randomly distributed bit file (i.e.50). So, using that as a point of comparison I would say that both algorithms perform well with regards to time. Additionally, neither algorithm is actually able to compress the randomly distributed file of 50% 0s and similar size, but the randomly distributed DNA and protein files are compressed to about a quarter (DNA) and 3/5 (protein) of their old size, demonstrating that these algorithms perform compression successfully as well.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressing real data\n",
    "\n",
    "**A priori, do you expect to achieve better or worse compression here than random data? Why?** I would expect better just because this is real data, and therefore should have patterns to the information that could be taken advantage of when compressing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "seqs = []\n",
    "\n",
    "handle = Entrez.esearch(db='nucleotide', term='gp120 HIV', sort='relevance', idtype='acc', retmax=10)\n",
    "for i in Entrez.read(handle)['IdList']:\n",
    "    handle = Entrez.efetch(db='nucleotide', id=i, rettype='fasta', retmode='text')\n",
    "    result = SeqIO.read(handle, \"fasta\")\n",
    "    handle.close()\n",
    "    names.append(result.name)\n",
    "    seqs.append(result.seq)\n",
    "    \n",
    "f = open('gp120_HIV.fa', 'w')\n",
    "for i in range(len(names)):\n",
    "    f.write('>' + names[i] + '\\n' + str(seqs[i]) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compress the FASTA file in terminal using gzip and bzip2: \n",
    "\n",
    "time gzip -c gp120_HIV.fa > gp120_HIV.fa.gz\n",
    "\n",
    "time bzip2 -k gp120_HIV.fa\n",
    "\n",
    "**How does the compression ratio of this file compare to random data?**\n",
    "The resulting size for gzip is 641B, and for bzip2 it is 778 B. The file starts of as being ~5.11 kB; therefore the rations are 8.16 and 6.73 for gzip and bzip2, respectively. Both compression ratios are significantly higher than that of the random data, which had an upper limit ratio of 3.66. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating compression of 1000 terabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given the benchmarking data you obtained in this lab, which algorithm do you propose to use for each type of data? Provide an estimate for the fraction of space you can save using your compression scheme. How much of a bonus do you anticipate receiving this year?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10% protein sequence:**\n",
    "\n",
    "pbzip2 both produces both the best compression level and the shortest compression time with protein sequences. The compression ratio achevied in the testing was around 1.75 . \n",
    "\n",
    "10% * 1000TB = 100TB ;  100 TB / 1.7 = 58.8 TB. So, I would estimate that we save 41.2 TB of data here.\n",
    "\n",
    "**80% DNA data:**\n",
    "\n",
    "Although bzip2 is better than gzip with simulated data, when working with real data gzip has a compression ratio of 8.16, higher than bzip2's 6.73. Obviously, our lab is dealing with real data, so this data should be valued more highly. Using gzip with an estimated compression ratio of 8.16 is what I'd reccomend, with the one drawback being that it would take a significant amount of time to run. \n",
    "\n",
    "80% * 1000TB = 800TB ; 800TB/8.16 ~ 98 TB, saving us 702 TB of data!\n",
    "\n",
    "**10% Randomly Distributed Binary Data**\n",
    "\n",
    "Our results from the \"50\" file of simulated binary data show that this cannot be compressed so I'm going to ignore it. \n",
    "\n",
    "Because of these compression algorithms, 742.2 TB of data are saved daily, which, taking into account that every TB costs 50/day, saves us $37160 a day. Multiplying this number by 253 working days demonstrates that our tem will receive a bonus of 9,401,480. I could use 9 million dollars. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
